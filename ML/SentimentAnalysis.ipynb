{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ee73a5",
   "metadata": {},
   "source": [
    "# Sentiment basé sur les reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c819a",
   "metadata": {},
   "source": [
    "J'ai testé différent Dataset avec différents modèle de text mining pour prédire les sentiments à partir des reviews.\n",
    "Une première approche en utilisant le Dataset twitter.\n",
    "Une deuxième approche en utilisant nos propres jeux de données Trustpilot\n",
    "Le modèle qui fonctionne le mieux pour le moment est le modèle bag CountVectorizer. Le modèle prédit assez bien pour les avis positifs et un peu moins bien pour les avis négatifs et neutre. \n",
    "Des solutions pour améliorer le modèle:\n",
    "* Ajout de données pour l'entrainement\n",
    "* Utiliser un modèle de text Mining Pré-entrainer sur un gros volume de données tel que Vader\n",
    "* Utiliser d'autre Dataset ou d'autres modèles de text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465ec04-2c77-4dc2-938a-59ff90d885ef",
   "metadata": {},
   "source": [
    "# Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c926b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "if nltk.download('punkt') == True:\n",
    "    pass\n",
    "else:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "if nltk.download('stopwords') == True:\n",
    "    pass\n",
    "else:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "if nltk.download('wordnet') == True:\n",
    "    pass\n",
    "else:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952744d",
   "metadata": {},
   "source": [
    "# Nettoyage et traitement jeu de données"
   ]
  },
  {
   "cell_type": "raw",
   "id": "526e8aad",
   "metadata": {},
   "source": [
    "Fonction qui permet de récuperer les tokens dans la reviews. On filtre pour garder les tokens seulement avec une longueur supérieur ou égale à 4 pour garder une cohérence dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03586430",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]{4,}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ae905b9",
   "metadata": {},
   "source": [
    "Fonction qui va permettre d'enlever les stopwords pour chaques reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d2cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\".\",\",\",\"?\",\"@\"])\n",
    "\n",
    "def stop_words_filtering(l):\n",
    "    for element in l:\n",
    "        if element in stop_words:\n",
    "            l.remove(element)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d47cc297",
   "metadata": {},
   "source": [
    "Fonction de lemmatisation qui permet de réduire le mot dans sa forme canonique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6e89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatisation(mots):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for element in mots:\n",
    "        radical = wordnet_lemmatizer.lemmatize(element, pos='v')\n",
    "        if (radical not in result):\n",
    "            result.append(radical)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff1cd01e",
   "metadata": {},
   "source": [
    "Fonction de stemmer pour réduire le mot à sa racine. on a décidé de ne pas l'utiliser car la lemmatisation est plus performante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feac862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(mots) :\n",
    "    stemmer = PorterStemmer()\n",
    "    sortie = []\n",
    "    for string in mots :\n",
    "        radical = stemmer.stem(string)\n",
    "        if (radical not in sortie) : sortie.append(radical)\n",
    "    return sortie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a117bb0",
   "metadata": {},
   "source": [
    "# Twitter Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73643240",
   "metadata": {},
   "source": [
    "Source https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2e154-6fcd-4282-a813-0021cc928ab6",
   "metadata": {},
   "source": [
    "L'objectif est de créer un modèle capable de prédire si un texte est positif neutre ou négatif. Pour cela on va prendre un jeu de donnée contenant des reviews avec leur label. Après cet entrainement, on va tester le modèle avec nos propres données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dfb8673-58a3-4c6d-b613-1d9171ce03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_twitter_data = f\"{os.getcwd()}/train_data/Twitter_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db2e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter = pd.read_csv(path_twitter_data, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7051bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter[\"category\"] = data_twitter[\"category\"].replace(to_replace = -1.0, value = \"negatif\")\n",
    "data_twitter[\"category\"] = data_twitter[\"category\"].replace(to_replace = 1.0, value = \"positif\")\n",
    "data_twitter[\"category\"] = data_twitter[\"category\"].replace(to_replace = 0.0, value = \"neutre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149828b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter = data_twitter.dropna()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c2ebf8c",
   "metadata": {},
   "source": [
    "On voit ici que les valeurs ne sont pas équilibré, pour éviter l'overfitting sur une valeur on va équilibré nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd655f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "positif    72249\n",
       "neutre     55211\n",
       "negatif    35509\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_twitter[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc89d4c5",
   "metadata": {},
   "source": [
    "On équilibre nos données en faisant du sous_échantillonnage en prenant la valeur \"negatif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a97e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter_positif = data_twitter[data_twitter['category'] == 'positif'].sample(n=35509, random_state=42)\n",
    "data_twitter_neutre = data_twitter[data_twitter['category'] == 'neutre'].sample(n=35509, random_state=42)\n",
    "data_twitter_negatif = data_twitter[data_twitter['category'] == 'negatif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a6f429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter = pd.concat([data_twitter_positif, data_twitter_negatif,data_twitter_neutre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "708a3306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "positif    35509\n",
       "negatif    35509\n",
       "neutre     35509\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_twitter[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee76b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a en tout 106527 lignes pour notre modèle\n"
     ]
    }
   ],
   "source": [
    "print(f\"On a en tout {len(data_twitter)} lignes pour notre modèle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc018448-5ac7-40ff-b909-c2e9530fdd77",
   "metadata": {},
   "source": [
    "# Algo BAG avec CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ae08c-ad4d-414f-90ec-dd81b188d71a",
   "metadata": {},
   "source": [
    "On va prendre importer nos données d'entrainements et on va ensuite diviser en jeu d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "966b170a-a005-4cc0-acbf-582b91ae7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"modelCV_twitter.pkl\"\n",
    "if os.path.exists(f\"{os.getcwd()}/model/{model_name}\"):\n",
    "\n",
    "    vectorizer, model_clf_CV_twitter = joblib.load(f\"{os.getcwd()}/model/{model_name}\")\n",
    "    X_test, y_test = joblib.load(f\"{os.getcwd()}/model/test_data_twitter_CV.pkl\")\n",
    "\n",
    "else:\n",
    "    X = data_twitter[\"clean_text\"]\n",
    "    y = data_twitter[\"category\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 30)\n",
    "\n",
    "    X_train = X_train.str.lower().apply(tokenizer.tokenize)\n",
    "    X_test = X_test.str.lower().apply(tokenizer.tokenize)\n",
    "    \n",
    "    X_train = X_train.apply(stop_words_filtering)\n",
    "    X_test = X_test.apply(stop_words_filtering)\n",
    "    \n",
    "    X_train = X_train.apply(lemmatisation)\n",
    "    X_test = X_test.apply(lemmatisation)\n",
    "    \n",
    "    X_train = X_train.apply(str)\n",
    "    X_test = X_test.apply(str)\n",
    "\n",
    "    #application du CountVectorizer sur nos jeux d'entrainements et de tests\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    model_clf_CV_twitter = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42).fit(X_train, y_train)\n",
    "    joblib.dump((vectorizer,model_clf_CV_twitter), f\"{os.getcwd()}/model/{model_name}\")\n",
    "    joblib.dump((X_test, y_test), f\"{os.getcwd()}/model/test_data_twitter_CV.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83677e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_clf_CV_twitter.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2b353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.85      0.72      0.78      7021\n",
      "      neutre       0.70      0.95      0.81      7200\n",
      "     positif       0.87      0.69      0.77      7085\n",
      "\n",
      "    accuracy                           0.79     21306\n",
      "   macro avg       0.81      0.79      0.79     21306\n",
      "weighted avg       0.81      0.79      0.79     21306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4d6f4-7e90-4ded-8d35-80de9e1a54ca",
   "metadata": {},
   "source": [
    "Cette fonction prends en entrée un pandas Serie de texte et le modèle et renvoie en sortie une liste avec nos prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bfd8dc4-23ef-4b1d-87e7-18a3d9872327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_cv(reviews,model_CV):\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]{4,}\")\n",
    "    result = []\n",
    "    reviews = reviews.str.lower().apply(tokenizer.tokenize)\n",
    "    reviews = reviews.apply(stop_words_filtering)\n",
    "    reviews = reviews.apply(lemmatisation)\n",
    "    reviews = reviews.apply(str)\n",
    "    for text in reviews:\n",
    "        new_text_vectorized = vectorizer.transform([text])\n",
    "\n",
    "        prediction = model_CV.predict(new_text_vectorized)\n",
    "        result.append(prediction[0])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc6724-ba69-44ea-bc8e-d89be5a327d2",
   "metadata": {},
   "source": [
    "# Algo BAG avec TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222003c-6f6c-44ef-a600-2b6abc566a62",
   "metadata": {},
   "source": [
    "La même chose avec tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a52ccf02-13e8-4985-8e4f-aa91a67564c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"modeltfidf_twitter.pkl\"\n",
    "if os.path.exists(f\"{os.getcwd()}/model/{model_name}\"):\n",
    "\n",
    "    vec_tfidf, model_CLF_tfidf_twitter = joblib.load(f\"{os.getcwd()}/model/{model_name}\")\n",
    "    X_test_tfidf, y_test_tfidf = joblib.load(f\"{os.getcwd()}/model/test_data_twitter_tfidf.pkl\")\n",
    "\n",
    "else:\n",
    "    X = data_twitter[\"clean_text\"]\n",
    "    y = data_twitter[\"category\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 30)\n",
    "\n",
    "    X_train = X_train.str.lower().apply(tokenizer.tokenize)\n",
    "    X_test = X_test.str.lower().apply(tokenizer.tokenize)\n",
    "    \n",
    "    X_train = X_train.apply(stop_words_filtering)\n",
    "    X_test = X_test.apply(stop_words_filtering)\n",
    "    \n",
    "    X_train = X_train.apply(lemmatisation)\n",
    "    X_test = X_test.apply(lemmatisation)\n",
    "    \n",
    "    X_train = X_train.apply(str)\n",
    "    X_test = X_test.apply(str)\n",
    "\n",
    "    #application du TfidfVectorizer sur nos jeux d'entrainements et de tests\n",
    "    vec_tfidf = TfidfVectorizer()\n",
    "    X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "    X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "\n",
    "    model_CLF_tfidf_twitter = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42).fit(X_train_tfidf, y_train)\n",
    "    joblib.dump((vec_tfidf, model_CLF_tfidf_twitter), f\"{os.getcwd()}/model/{model_name}\")\n",
    "    joblib.dump((X_test_tfidf, y_test), f\"{os.getcwd()}/model/test_data_twitter_tfidf.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "215ac92c-e312-4114-8d74-5851a1ef2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf = model_CLF_tfidf_twitter.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c103695-6fd3-460f-aa01-948640f0e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.83      0.71      0.76      7021\n",
      "      neutre       0.70      0.95      0.80      7200\n",
      "     positif       0.87      0.67      0.75      7085\n",
      "\n",
      "    accuracy                           0.78     21306\n",
      "   macro avg       0.80      0.78      0.77     21306\n",
      "weighted avg       0.80      0.78      0.77     21306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fae2ed38-be6e-4782-b26d-7c641a6938bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_tfidf(reviews,model_tfidf):\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]{4,}\")\n",
    "    result = []\n",
    "    reviews = reviews.str.lower().apply(tokenizer.tokenize)\n",
    "    reviews = reviews.apply(stop_words_filtering)\n",
    "    reviews = reviews.apply(lemmatisation)\n",
    "    reviews = reviews.apply(str)\n",
    "    for text in reviews:\n",
    "\n",
    "        new_text_vectorized = vec_tfidf.transform([text])\n",
    "\n",
    "        prediction = model_tfidf.predict(new_text_vectorized)\n",
    "        result.append(prediction[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f709c6-d115-422a-98e8-678ce8a8160a",
   "metadata": {},
   "source": [
    "# Sentiments Analysis avec Trustpilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb00487-bf0c-41cc-ada2-9b5f5633d4e1",
   "metadata": {},
   "source": [
    "On va se servir de nos propres jeux de données truspilot dans ce cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fff03658-d326-4973-ae34-03ca7d77d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_trustpilot_data = f\"{os.getcwd()}/train_data/firms_with_reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9736aaa2-141c-4ab6-be6f-bd411bc05c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_trustpilot_data, index_col=0 ,encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d905cf9-2c7d-45ed-81a0-eacc1a5325de",
   "metadata": {},
   "source": [
    "Ici nous avons entrainé notre modèle avec nos propres données que l'on a scrappé sur truspilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490930c2-5223-48c6-9f3b-74e0f2ef8235",
   "metadata": {},
   "source": [
    "Un petit aperçu de nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40377b4b-6e83-468b-99ec-4c32f09a7c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_title</th>\n",
       "      <th>author_id</th>\n",
       "      <th>experience_date</th>\n",
       "      <th>extract_date</th>\n",
       "      <th>firm_id</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_note</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_url</th>\n",
       "      <th>author_localisation</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_url</th>\n",
       "      <th>firm_name</th>\n",
       "      <th>reponse</th>\n",
       "      <th>note</th>\n",
       "      <th>firm_info</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66ccad2d63a995b7aa02bc03</th>\n",
       "      <td>Alvaro made my experience veryâ¦</td>\n",
       "      <td>66ccad2d63a995b7aa02bc01</td>\n",
       "      <td>2023-05-22T00:00:00</td>\n",
       "      <td>2024-08-23T00:00:00</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>2023-05-23T00:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Alvaro made my experience very satisfactory! I...</td>\n",
       "      <td>/reviews/646cba0dc423446286686604</td>\n",
       "      <td>US</td>\n",
       "      <td>Keontra Reid</td>\n",
       "      <td>/users/646cba0ba8905b00124cdbfb</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'_id': '66db33a2bd030051b2456e59', 'page_url'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66ccad2d63a995b7aa02bc08</th>\n",
       "      <td>Great company to work with they reallyâ¦</td>\n",
       "      <td>66ccad2d63a995b7aa02bc06</td>\n",
       "      <td>2023-05-22T00:00:00</td>\n",
       "      <td>2024-08-23T00:00:00</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>2023-05-23T00:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great company to work with they really underst...</td>\n",
       "      <td>/reviews/646cad6cc423446286685c19</td>\n",
       "      <td>US</td>\n",
       "      <td>Ismael Luciano</td>\n",
       "      <td>/users/646cad6b05330f0014134602</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'_id': '66db33a2bd030051b2456e59', 'page_url'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66ccad2d63a995b7aa02bc0d</th>\n",
       "      <td>HELPFUL..</td>\n",
       "      <td>66ccad2d63a995b7aa02bc0b</td>\n",
       "      <td>2023-05-22T00:00:00</td>\n",
       "      <td>2024-08-23T00:00:00</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>2023-05-23T00:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>HELPFUL... HONEST...TRUTHUL!!!!!!!!!!!!!!!!!!!</td>\n",
       "      <td>/reviews/646c8127706f837cb1eff2f6</td>\n",
       "      <td>US</td>\n",
       "      <td>Tony RODRIGUEZ</td>\n",
       "      <td>/users/646c8126a8905b00124cad7c</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'_id': '66db33a2bd030051b2456e59', 'page_url'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66ccad2d63a995b7aa02bc12</th>\n",
       "      <td>Making it very easy to understand andâ¦</td>\n",
       "      <td>66ccad2d63a995b7aa02bc10</td>\n",
       "      <td>2023-05-22T00:00:00</td>\n",
       "      <td>2024-08-23T00:00:00</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>2023-05-23T00:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Making it very easy to understand and answerin...</td>\n",
       "      <td>/reviews/646c6c27706f837cb1efe4ad</td>\n",
       "      <td>US</td>\n",
       "      <td>Melinda Hall</td>\n",
       "      <td>/users/646c6c254be4ac0013350e3c</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'_id': '66db33a2bd030051b2456e59', 'page_url'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66ccad2d63a995b7aa02bc17</th>\n",
       "      <td>Leif was awesome</td>\n",
       "      <td>66ccad2d63a995b7aa02bc15</td>\n",
       "      <td>2023-05-22T00:00:00</td>\n",
       "      <td>2024-08-23T00:00:00</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>2023-05-23T00:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Leif was awesome. Hes so friendly, easy to tal...</td>\n",
       "      <td>/reviews/646c4e46706f837cb1efd615</td>\n",
       "      <td>US</td>\n",
       "      <td>SDS</td>\n",
       "      <td>/users/646c4e454be4ac001334fb5f</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'_id': '66db33a2bd030051b2456e59', 'page_url'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       review_title  \\\n",
       "_id                                                                   \n",
       "66ccad2d63a995b7aa02bc03          Alvaro made my experience veryâ¦   \n",
       "66ccad2d63a995b7aa02bc08  Great company to work with they reallyâ¦   \n",
       "66ccad2d63a995b7aa02bc0d                                  HELPFUL..   \n",
       "66ccad2d63a995b7aa02bc12   Making it very easy to understand andâ¦   \n",
       "66ccad2d63a995b7aa02bc17                           Leif was awesome   \n",
       "\n",
       "                                         author_id      experience_date  \\\n",
       "_id                                                                       \n",
       "66ccad2d63a995b7aa02bc03  66ccad2d63a995b7aa02bc01  2023-05-22T00:00:00   \n",
       "66ccad2d63a995b7aa02bc08  66ccad2d63a995b7aa02bc06  2023-05-22T00:00:00   \n",
       "66ccad2d63a995b7aa02bc0d  66ccad2d63a995b7aa02bc0b  2023-05-22T00:00:00   \n",
       "66ccad2d63a995b7aa02bc12  66ccad2d63a995b7aa02bc10  2023-05-22T00:00:00   \n",
       "66ccad2d63a995b7aa02bc17  66ccad2d63a995b7aa02bc15  2023-05-22T00:00:00   \n",
       "\n",
       "                                 extract_date        firm_id  \\\n",
       "_id                                                            \n",
       "66ccad2d63a995b7aa02bc03  2024-08-23T00:00:00  turbodebt.com   \n",
       "66ccad2d63a995b7aa02bc08  2024-08-23T00:00:00  turbodebt.com   \n",
       "66ccad2d63a995b7aa02bc0d  2024-08-23T00:00:00  turbodebt.com   \n",
       "66ccad2d63a995b7aa02bc12  2024-08-23T00:00:00  turbodebt.com   \n",
       "66ccad2d63a995b7aa02bc17  2024-08-23T00:00:00  turbodebt.com   \n",
       "\n",
       "                                  review_date  review_note  \\\n",
       "_id                                                          \n",
       "66ccad2d63a995b7aa02bc03  2023-05-23T00:00:00          5.0   \n",
       "66ccad2d63a995b7aa02bc08  2023-05-23T00:00:00          5.0   \n",
       "66ccad2d63a995b7aa02bc0d  2023-05-23T00:00:00          5.0   \n",
       "66ccad2d63a995b7aa02bc12  2023-05-23T00:00:00          5.0   \n",
       "66ccad2d63a995b7aa02bc17  2023-05-23T00:00:00          5.0   \n",
       "\n",
       "                                                                review_text  \\\n",
       "_id                                                                           \n",
       "66ccad2d63a995b7aa02bc03  Alvaro made my experience very satisfactory! I...   \n",
       "66ccad2d63a995b7aa02bc08  Great company to work with they really underst...   \n",
       "66ccad2d63a995b7aa02bc0d     HELPFUL... HONEST...TRUTHUL!!!!!!!!!!!!!!!!!!!   \n",
       "66ccad2d63a995b7aa02bc12  Making it very easy to understand and answerin...   \n",
       "66ccad2d63a995b7aa02bc17  Leif was awesome. Hes so friendly, easy to tal...   \n",
       "\n",
       "                                                 review_url  \\\n",
       "_id                                                           \n",
       "66ccad2d63a995b7aa02bc03  /reviews/646cba0dc423446286686604   \n",
       "66ccad2d63a995b7aa02bc08  /reviews/646cad6cc423446286685c19   \n",
       "66ccad2d63a995b7aa02bc0d  /reviews/646c8127706f837cb1eff2f6   \n",
       "66ccad2d63a995b7aa02bc12  /reviews/646c6c27706f837cb1efe4ad   \n",
       "66ccad2d63a995b7aa02bc17  /reviews/646c4e46706f837cb1efd615   \n",
       "\n",
       "                         author_localisation     author_name  \\\n",
       "_id                                                            \n",
       "66ccad2d63a995b7aa02bc03                  US    Keontra Reid   \n",
       "66ccad2d63a995b7aa02bc08                  US  Ismael Luciano   \n",
       "66ccad2d63a995b7aa02bc0d                  US  Tony RODRIGUEZ   \n",
       "66ccad2d63a995b7aa02bc12                  US    Melinda Hall   \n",
       "66ccad2d63a995b7aa02bc17                  US             SDS   \n",
       "\n",
       "                                               author_url  firm_name  reponse  \\\n",
       "_id                                                                             \n",
       "66ccad2d63a995b7aa02bc03  /users/646cba0ba8905b00124cdbfb  TurboDebt     True   \n",
       "66ccad2d63a995b7aa02bc08  /users/646cad6b05330f0014134602  TurboDebt     True   \n",
       "66ccad2d63a995b7aa02bc0d  /users/646c8126a8905b00124cad7c  TurboDebt     True   \n",
       "66ccad2d63a995b7aa02bc12  /users/646c6c254be4ac0013350e3c  TurboDebt     True   \n",
       "66ccad2d63a995b7aa02bc17  /users/646c4e454be4ac001334fb5f  TurboDebt     True   \n",
       "\n",
       "                          note  \\\n",
       "_id                              \n",
       "66ccad2d63a995b7aa02bc03   5.0   \n",
       "66ccad2d63a995b7aa02bc08   5.0   \n",
       "66ccad2d63a995b7aa02bc0d   5.0   \n",
       "66ccad2d63a995b7aa02bc12   5.0   \n",
       "66ccad2d63a995b7aa02bc17   5.0   \n",
       "\n",
       "                                                                  firm_info  \n",
       "_id                                                                          \n",
       "66ccad2d63a995b7aa02bc03  {'_id': '66db33a2bd030051b2456e59', 'page_url'...  \n",
       "66ccad2d63a995b7aa02bc08  {'_id': '66db33a2bd030051b2456e59', 'page_url'...  \n",
       "66ccad2d63a995b7aa02bc0d  {'_id': '66db33a2bd030051b2456e59', 'page_url'...  \n",
       "66ccad2d63a995b7aa02bc12  {'_id': '66db33a2bd030051b2456e59', 'page_url'...  \n",
       "66ccad2d63a995b7aa02bc17  {'_id': '66db33a2bd030051b2456e59', 'page_url'...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddbb7232-fb9d-4edf-980f-bfe01e5f06fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons en tout 214647 lignes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nous avons en tout {len(df)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ec7239b-6c8a-49f0-a772-7581086bef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après avoir supprimé les NaN nous avons 188096 lignes\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(f\"Après avoir supprimé les NaN nous avons {len(df)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2358717-5922-4970-a114-a1c88744bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiments\"] = df[\"note\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8407e99b-e243-4beb-becb-00fefb8c942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiments\"] = df[\"sentiments\"].replace(to_replace = [2.0,1.0], value = \"negatif\")\n",
    "df[\"sentiments\"] = df[\"sentiments\"].replace(to_replace = [4.0,5.0], value = \"positif\")\n",
    "df[\"sentiments\"] = df[\"sentiments\"].replace(to_replace = [3.0], value = \"neutre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "671eea05-2de4-4f65-9246-f9fe1659f1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiments\n",
       "positif    176457\n",
       "negatif      7323\n",
       "neutre       4316\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiments\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd180a2e-a4a7-4543-bacb-3b63dd528643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positif = df[df['sentiments'] == 'positif'].sample(n=8000, random_state=42)\n",
    "df_negatif = df[df['sentiments'] == 'negatif']\n",
    "df_neutre = df[df['sentiments'] == 'neutre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7427ce05-ddc7-49b0-9b69-d3dbe406128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_positif, df_negatif, df_neutre])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e761a37-c3b5-4f4c-8dd3-c80432089f51",
   "metadata": {},
   "source": [
    "Notre jeux de données est déjà plus équilibré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c290d7b-741c-4536-9632-70174d843046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiments\n",
       "positif    8000\n",
       "negatif    7323\n",
       "neutre     4316\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiments\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49907b9f-bb20-4dac-a179-8cdce1c5f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"modelCV_trustpilot.pkl\"\n",
    "if os.path.exists(f\"{os.getcwd()}/model/{model_name}\"):\n",
    "\n",
    "    vectorizer, model_clf_CV_trustpilot = joblib.load(f\"{os.getcwd()}/model/{model_name}\")\n",
    "    X_test, y_test = joblib.load(f\"{os.getcwd()}/model/test_data_trustpilot_CV.pkl\")\n",
    "\n",
    "else:\n",
    "    X = df[\"review_text\"]\n",
    "    y = df[\"sentiments\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 30)\n",
    "\n",
    "    X_train = X_train.str.lower().apply(tokenizer.tokenize)\n",
    "    X_test = X_test.str.lower().apply(tokenizer.tokenize)\n",
    "    \n",
    "    X_train = X_train.apply(stop_words_filtering)\n",
    "    X_test = X_test.apply(stop_words_filtering)\n",
    "        \n",
    "    X_train = X_train.apply(lemmatisation)\n",
    "    X_test = X_test.apply(lemmatisation)\n",
    "        \n",
    "    X_train = X_train.apply(str)\n",
    "    X_test = X_test.apply(str)\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    model_clf_CV_trustpilot = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42).fit(X_train, y_train)\n",
    "    joblib.dump((vectorizer,model_clf_CV_trustpilot), f\"{os.getcwd()}/model/{model_name}\")\n",
    "    joblib.dump((X_test, y_test), f\"{os.getcwd()}/model/test_data_trustpilot_CV.pkl\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "579e86f2-1a1e-42ac-8a35-e95c3f495a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_clf_CV_trustpilot.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50971a52-ee59-4f51-8e02-92186a6a2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.73      0.76      0.74      1435\n",
      "      neutre       0.55      0.36      0.44       888\n",
      "     positif       0.79      0.91      0.84      1605\n",
      "\n",
      "    accuracy                           0.73      3928\n",
      "   macro avg       0.69      0.68      0.68      3928\n",
      "weighted avg       0.71      0.73      0.72      3928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aec1a30-1383-48a7-ad18-626820b58c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7309063136456212"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred,output_dict=True)[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3010cca-bfaa-4d11-9b58-56326094799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3928.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_metric_train = classification_report(y_test, y_pred,output_dict=True)[\"weighted avg\"]\n",
    "report_metric_train.pop(\"support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca57310d-ed05-4242-8d7a-07ceb04880cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7135863961667934,\n",
       " 'recall': 0.7309063136456212,\n",
       " 'f1-score': 0.7159440454624767}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_metric_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9086f19e-8b6a-496c-a7a9-bfc66d882de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19639"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9eecfa3-a306-49aa-9c34-9776c9a05b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_date  = '2024-10-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec4f38fe-9e9e-47cd-bd67-887eb3aedb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "l = [current_date, accuracy, precision, recall, f1]\n",
    "columns = ['date', 'accuracy', 'precision', 'recall', 'f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcacc6e9-d05f-4ee1-9ea7-217f58cd5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame([l], columns=columns)\n",
    "metrics_df.to_csv('metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbce5f-07c0-40c8-bfbe-a1c7de9feddf",
   "metadata": {},
   "source": [
    "# Test du modèle avec nos jeux de données sur mongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c35820-bd90-4254-9450-0206e66230b2",
   "metadata": {},
   "source": [
    "Une fois qu'on a notre modèle on l'utiliser sur nos données scrappés qui se trouvent dans la base mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afefc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\n",
    "    host = \"my_mongo_server\",\n",
    "    port = 27017,\n",
    "    username = \"datascientest\",\n",
    "    password = \"dst123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0744b65-7846-456e-b251-2c8a583f7e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reviews', 'Firms']\n"
     ]
    }
   ],
   "source": [
    "pprint(client[\"Projet\"].list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c7dd1b9-ffee-4f58-b5d3-62b5f2fd3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('66fea989b80f255a7a991a18'),\n",
      " 'author_localisation': 'US',\n",
      " 'author_name': 'Keontra Reid',\n",
      " 'author_url': '/users/646cba0ba8905b00124cdbfb',\n",
      " 'experience_date': datetime.datetime(2023, 5, 22, 0, 0),\n",
      " 'extract_date': datetime.datetime(2024, 8, 23, 0, 0),\n",
      " 'firm_id': 'turbodebt.com',\n",
      " 'firm_name': 'TurboDebt',\n",
      " 'note': 5.0,\n",
      " 'reponse': 'True',\n",
      " 'review_date': datetime.datetime(2023, 5, 23, 0, 0),\n",
      " 'review_text': 'Alvaro made my experience very satisfactory! I felt like I '\n",
      "                'could breathe again after speaking with him.',\n",
      " 'review_title': 'Alvaro made my experience very…',\n",
      " 'review_url': '/reviews/646cba0dc423446286686604'}\n"
     ]
    }
   ],
   "source": [
    "pprint(client[\"Projet\"]['Reviews'].find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9954bd75-7528-46c1-b4e4-755b7c9f918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Projet', 'admin', 'config', 'local']\n"
     ]
    }
   ],
   "source": [
    "print(client.list_database_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6cc95-97b4-48c0-a34f-374d7d51693d",
   "metadata": {},
   "source": [
    "Liste des colonnes de notre jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7bb1122-6855-40ae-b91b-239843fabe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'review_url', 'author_localisation', 'author_name', 'author_url', 'experience_date', 'extract_date', 'firm_id', 'firm_name', 'note', 'reponse', 'review_date', 'review_text', 'review_title'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client[\"Projet\"][\"Reviews\"].find_one().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9725de9a-baf1-4302-9188-b286402e3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_cv(reviews,model_CV):\n",
    "    result = []\n",
    "    reviews = reviews.str.lower().apply(tokenizer.tokenize)\n",
    "    reviews = reviews.apply(stop_words_filtering)\n",
    "    reviews = reviews.apply(lemmatisation)\n",
    "    reviews = reviews.apply(str)\n",
    "    for text in reviews:\n",
    "        new_text_vectorized = vectorizer.transform([text])\n",
    "\n",
    "        prediction =  model_clf_CV_trustpilot.predict(new_text_vectorized)\n",
    "        result.append(prediction[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57a5e28c-7769-4657-8335-46acc436f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98c82d72-14d9-4abc-a89d-c8eabb1d4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:5000].to_csv('data_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42860edd-81c3-4af7-908e-0371463f1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_existant = pd.read_csv('data_final.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84510d18-9404-4a69-bbc0-d3c8510be33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "existant_ids = set(data_existant['review_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd9b39b1-044f-4669-a1e9-6353b166f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_metric = client[\"Projet\"][\"Reviews\"].find({\"review_url\": {\"$nin\": list(existant_ids)}},{\"_id\": 0, 'Unnamed: 0': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e774215b-814e-48a8-96fc-4a197f2964fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_data_metric = pd.DataFrame(list(new_data_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8aa5a6f2-d7ff-415e-bd84-ebad7f887163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"note\"]\n",
    "df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"sentiments\"].replace(to_replace = [2.0,1.0], value = \"negatif\")\n",
    "df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"sentiments\"].replace(to_replace = [4.0,5.0], value = \"positif\")\n",
    "df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"sentiments\"].replace(to_replace = [3.0], value = \"neutre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54f38afa-004c-404f-bf47-2b71302ba110",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_new_data_metric[\"sentiments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c733863-fa64-41b1-940d-4ba9249602b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = extract_sentiment_cv(df_new_data_metric[\"review_text\"], model_clf_CV_trustpilot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "407d5137-114c-46aa-8b97-1182df0e2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46d33f7e-2742-475b-86be-65c8b228391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97a5500b-30db-4a6d-851c-766aac078224",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_metrics = {\n",
    "    'date': current_date,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db575746-cabd-41c6-add8-31ae68f6cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_metrics = pd.DataFrame([new_data_metrics]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d56dbee-3f85-4560-8bb9-93a6d9877c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.concat([metrics_df, new_data_metrics], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c0bcd8e-286b-46e1-aea9-e90e6776afdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>0.730906</td>\n",
       "      <td>0.713586</td>\n",
       "      <td>0.730906</td>\n",
       "      <td>0.715944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-03</td>\n",
       "      <td>0.899154</td>\n",
       "      <td>0.946096</td>\n",
       "      <td>0.899154</td>\n",
       "      <td>0.918364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  accuracy  precision    recall  f1_score\n",
       "0  2024-10-01  0.730906   0.713586  0.730906  0.715944\n",
       "1  2024-10-03  0.899154   0.946096  0.899154  0.918364"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "309f23f9-37f8-4dd0-b08b-4a21308b2d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e9966c4-b656-484a-b74c-55612c8f6531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le fichier existe et il y a de nouvelles données\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(f\"{os.getcwd()}/data_final.csv\"):\n",
    "    data_existing = pd.read_csv('data_final.csv')   \n",
    "    existing_ids = set(data_existing['review_url'])\n",
    "    new_data = client[\"Projet\"][\"Reviews\"].find({\"review_url\": {\"$nin\": list(existing_ids)}},{\"_id\": 0, 'Unnamed: 0': 0})\n",
    "    df_new_data = pd.DataFrame(list(new_data))\n",
    "    df_new_data_metric = df_new_data.copy()\n",
    "    if len(df_new_data) == 0:\n",
    "        print(\"Aucune nouvelles données\")\n",
    "    else:\n",
    "        print(\"le fichier existe et il y a de nouvelles données\")\n",
    "        df_new_data[\"sentiments\"] = extract_sentiment_cv(df_new_data[\"review_text\"], model_clf_CV_trustpilot)\n",
    "        y_pred = df_new_data[\"sentiments\"].copy()\n",
    "        df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"note\"]\n",
    "        df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"sentiments\"].replace(to_replace = [2.0,1.0], value = \"negatif\")\n",
    "        df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"sentiments\"].replace(to_replace = [4.0,5.0], value = \"positif\")\n",
    "        df_new_data_metric[\"sentiments\"] = df_new_data_metric[\"sentiments\"].replace(to_replace = [3.0], value = \"neutre\")\n",
    "        y_test = df_new_data_metric[\"sentiments\"]\n",
    "        #y_pred = extract_sentiment_cv(df_new_data_metric[\"review_text\"], model_clf_CV_trustpilot)\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        new_data_metrics = {\n",
    "            'date': current_date,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "        new_data_metrics = pd.DataFrame([new_data_metrics]) \n",
    "        metrics_df = pd.concat([metrics_df, new_data_metrics], ignore_index=True)\n",
    "        df_new_data['review_text'] = df_new_data['review_text'].replace('\\r', ' ', regex=True)\n",
    "        df_new_data[\"reponse\"] = df_new_data[\"reponse\"].astype(bool)\n",
    "        df_new_data[\"reponse\"] = df_new_data[\"reponse\"].replace({True: \"true\", False: \"false\"})\n",
    "        data = pd.concat([data_existing, df_new_data], ignore_index=True)\n",
    "        data[\"reponse\"] = data[\"reponse\"].replace({True: \"true\", False: \"false\"})\n",
    "        data['experience_date'] = pd.to_datetime(data['experience_date']).dt.strftime('%Y-%m-%d')\n",
    "        data['extract_date'] = pd.to_datetime(data['extract_date']).dt.strftime('%Y-%m-%d')\n",
    "        data['review_date'] = pd.to_datetime(data['review_date']).dt.strftime('%Y-%m-%d')\n",
    "        data.to_csv('data_final.csv', index = False)\n",
    "        metrics_df.to_csv('metrics.csv', index = False)\n",
    "else:\n",
    "    print(\"data_final.csv n'existe pas \")\n",
    "    full_data = []\n",
    "    for element in client[\"Projet\"][\"Reviews\"].find({},{ \"_id\":0, 'Unnamed: 0':0 }):\n",
    "        full_data.append(element)\n",
    "    data = pd.DataFrame(full_data)\n",
    "    data = data.dropna(subset=['review_text'])\n",
    "    data[\"sentiments\"] = extract_sentiment_cv(data[\"review_text\"], model_clf_CV_trustpilot)\n",
    "    data['review_text'] = data['review_text'].replace('\\r', ' ', regex=True)\n",
    "    data[\"reponse\"] = data[\"reponse\"].astype(bool)\n",
    "    #data[\"reponse\"] = data[\"reponse\"].replace(to_replace = True, value = \"true\")\n",
    "    #data[\"reponse\"] = data[\"reponse\"].replace(to_replace = False, value = \"false\")\n",
    "    data[\"reponse\"] = data[\"reponse\"].replace({True: \"true\", False: \"false\"})\n",
    "    data['experience_date'] = pd.to_datetime(data['experience_date']).dt.strftime('%Y-%m-%d')\n",
    "    data['extract_date'] = pd.to_datetime(data['extract_date']).dt.strftime('%Y-%m-%d')\n",
    "    data['review_date'] = pd.to_datetime(data['review_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    data.to_csv('data_final.csv', index = False)\n",
    "    metrics_df.to_csv('metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fd752-efd6-443b-814f-70b44c16f7eb",
   "metadata": {},
   "source": [
    "if os.path.isfile(f\"{os.getcwd()}/data_final.csv\"):\n",
    "    data_existing = pd.read_csv('data_final.csv')   \n",
    "    existing_ids = set(data_existing['review_url'])\n",
    "    new_data = client[\"Projet\"][\"Reviews\"].find({\"review_url\": {\"$nin\": list(existing_ids)}},{\"_id\": 0, 'Unnamed: 0': 0})\n",
    "    df_new_data = pd.DataFrame(list(new_data))\n",
    "    if len(df_new_data) == 0:\n",
    "        print(\"Aucune nouvelles données\")\n",
    "    else:\n",
    "        print(\"le fichier existe et il y a de nouvelles données\")\n",
    "        df_new_data[\"sentiments\"] = extract_sentiment_cv(df_new_data[\"review_text\"], model_clf_CV_trustpilot)\n",
    "        df_new_data['review_text'] = df_new_data['review_text'].replace('\\r', ' ', regex=True)\n",
    "        df_new_data[\"reponse\"] = df_new_data[\"reponse\"].astype(bool)\n",
    "        df_new_data[\"reponse\"] = df_new_data[\"reponse\"].replace({True: \"true\", False: \"false\"})\n",
    "        data = pd.concat([data_existing, df_new_data], ignore_index=True)\n",
    "        data[\"reponse\"] = data[\"reponse\"].replace({True: \"true\", False: \"false\"})\n",
    "        data['experience_date'] = pd.to_datetime(data['experience_date']).dt.strftime('%Y-%m-%d')\n",
    "        data['extract_date'] = pd.to_datetime(data['extract_date']).dt.strftime('%Y-%m-%d')\n",
    "        data['review_date'] = pd.to_datetime(data['review_date']).dt.strftime('%Y-%m-%d')\n",
    "        data.to_csv('data_final.csv', index = False)\n",
    "else:\n",
    "    print(\"data_final.csv n'existe pas \")\n",
    "    full_data = []\n",
    "    for element in client[\"Projet\"][\"Reviews\"].find({},{ \"_id\":0, 'Unnamed: 0':0 }):\n",
    "        full_data.append(element)\n",
    "    data = pd.DataFrame(full_data)\n",
    "    data = data.dropna(subset=['review_text'])\n",
    "    data[\"sentiments\"] = extract_sentiment_cv(data[\"review_text\"], model_clf_CV_trustpilot)\n",
    "    data['review_text'] = data['review_text'].replace('\\r', ' ', regex=True)\n",
    "    data[\"reponse\"] = data[\"reponse\"].astype(bool)\n",
    "    #data[\"reponse\"] = data[\"reponse\"].replace(to_replace = True, value = \"true\")\n",
    "    #data[\"reponse\"] = data[\"reponse\"].replace(to_replace = False, value = \"false\")\n",
    "    data[\"reponse\"] = data[\"reponse\"].replace({True: \"true\", False: \"false\"})\n",
    "    data['experience_date'] = pd.to_datetime(data['experience_date']).dt.strftime('%Y-%m-%d')\n",
    "    data['extract_date'] = pd.to_datetime(data['extract_date']).dt.strftime('%Y-%m-%d')\n",
    "    data['review_date'] = pd.to_datetime(data['review_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    data.to_csv('data_final.csv', index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d347c992-2b11-4f13-abba-47389219ce22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_url</th>\n",
       "      <th>author_localisation</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_url</th>\n",
       "      <th>experience_date</th>\n",
       "      <th>extract_date</th>\n",
       "      <th>firm_id</th>\n",
       "      <th>firm_name</th>\n",
       "      <th>note</th>\n",
       "      <th>reponse</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_title</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/reviews/646cba0dc423446286686604</td>\n",
       "      <td>US</td>\n",
       "      <td>Keontra Reid</td>\n",
       "      <td>/users/646cba0ba8905b00124cdbfb</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2024-08-23</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Alvaro made my experience very satisfactory! I...</td>\n",
       "      <td>Alvaro made my experience very…</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/reviews/646cad6cc423446286685c19</td>\n",
       "      <td>US</td>\n",
       "      <td>Ismael Luciano</td>\n",
       "      <td>/users/646cad6b05330f0014134602</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2024-08-23</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Great company to work with they really underst...</td>\n",
       "      <td>Great company to work with they really…</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/reviews/646c8127706f837cb1eff2f6</td>\n",
       "      <td>US</td>\n",
       "      <td>Tony RODRIGUEZ</td>\n",
       "      <td>/users/646c8126a8905b00124cad7c</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2024-08-23</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>HELPFUL... HONEST...TRUTHUL!!!!!!!!!!!!!!!!!!!</td>\n",
       "      <td>HELPFUL..</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/reviews/646c6c27706f837cb1efe4ad</td>\n",
       "      <td>US</td>\n",
       "      <td>Melinda Hall</td>\n",
       "      <td>/users/646c6c254be4ac0013350e3c</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2024-08-23</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Making it very easy to understand and answerin...</td>\n",
       "      <td>Making it very easy to understand and…</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/reviews/646c4e46706f837cb1efd615</td>\n",
       "      <td>US</td>\n",
       "      <td>SDS</td>\n",
       "      <td>/users/646c4e454be4ac001334fb5f</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2024-08-23</td>\n",
       "      <td>turbodebt.com</td>\n",
       "      <td>TurboDebt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>true</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Leif was awesome. Hes so friendly, easy to tal...</td>\n",
       "      <td>Leif was awesome</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_url author_localisation     author_name  \\\n",
       "0  /reviews/646cba0dc423446286686604                  US    Keontra Reid   \n",
       "1  /reviews/646cad6cc423446286685c19                  US  Ismael Luciano   \n",
       "2  /reviews/646c8127706f837cb1eff2f6                  US  Tony RODRIGUEZ   \n",
       "3  /reviews/646c6c27706f837cb1efe4ad                  US    Melinda Hall   \n",
       "4  /reviews/646c4e46706f837cb1efd615                  US             SDS   \n",
       "\n",
       "                        author_url experience_date extract_date  \\\n",
       "0  /users/646cba0ba8905b00124cdbfb      2023-05-22   2024-08-23   \n",
       "1  /users/646cad6b05330f0014134602      2023-05-22   2024-08-23   \n",
       "2  /users/646c8126a8905b00124cad7c      2023-05-22   2024-08-23   \n",
       "3  /users/646c6c254be4ac0013350e3c      2023-05-22   2024-08-23   \n",
       "4  /users/646c4e454be4ac001334fb5f      2023-05-22   2024-08-23   \n",
       "\n",
       "         firm_id  firm_name  note reponse review_date  \\\n",
       "0  turbodebt.com  TurboDebt   5.0    true  2023-05-23   \n",
       "1  turbodebt.com  TurboDebt   5.0    true  2023-05-23   \n",
       "2  turbodebt.com  TurboDebt   5.0    true  2023-05-23   \n",
       "3  turbodebt.com  TurboDebt   5.0    true  2023-05-23   \n",
       "4  turbodebt.com  TurboDebt   5.0    true  2023-05-23   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  Alvaro made my experience very satisfactory! I...   \n",
       "1  Great company to work with they really underst...   \n",
       "2     HELPFUL... HONEST...TRUTHUL!!!!!!!!!!!!!!!!!!!   \n",
       "3  Making it very easy to understand and answerin...   \n",
       "4  Leif was awesome. Hes so friendly, easy to tal...   \n",
       "\n",
       "                              review_title sentiments  \n",
       "0          Alvaro made my experience very…    positif  \n",
       "1  Great company to work with they really…    positif  \n",
       "2                                HELPFUL..    positif  \n",
       "3   Making it very easy to understand and…    positif  \n",
       "4                         Leif was awesome    positif  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e07fefa-4a91-4ac3-b5ba-1b5f0b6a4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152/2586677827.py:6: DeprecationWarning: The 'timeout' parameter is deprecated in favor of 'request_timeout'\n",
      "  es = Elasticsearch(hosts = \"http://es-container:9200\", timeout=3000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'index 'test_final' existe déjà.\n",
      "Documents ajoutés avec succès.\n",
      "L'index 'test_metrics' existe déjà.\n",
      "Documents ajoutés avec succès.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk, BulkIndexError \n",
    "import csv\n",
    "import os \n",
    "\n",
    "es = Elasticsearch(hosts = \"http://es-container:9200\", timeout=3000000)\n",
    "\n",
    "# Définir le nom de l'index\n",
    "index_final = \"test_final\"\n",
    "index_metrics = \"test_metrics\"\n",
    "data_file = \"data_final.csv\"\n",
    "metrics_file = \"metrics.csv\"\n",
    "# Définir le mapping pour l'index\n",
    "mapping_data = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"author_localisation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"author_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"author_url\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"experience_date\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd\"\n",
    "            },\n",
    "            \"extract_date\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd\"\n",
    "            },\n",
    "            \"firm_id\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"firm_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"note\": {\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"reponse\": {\n",
    "                \"type\": \"boolean\"\n",
    "            },\n",
    "            \"review_date\": {\n",
    "                \"type\": \"date\"\n",
    "            },\n",
    "            \"review_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"review_title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"review_url\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"sentiments\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mapping_metrics = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"experience_date\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd\"\n",
    "            },\n",
    "            \"accuracy\": {\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"precision\": {\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"recall\": {\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"f1_score\": {\n",
    "                \"type\": \"float\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "mapping = [mapping_data, mapping_metrics]\n",
    "index_name = [index_final, index_metrics]\n",
    "file_name = [data_file, metrics_file]\n",
    "\n",
    "for mapp, index, file in zip(mapping,index_name, file_name):\n",
    "    # Créer l'index\n",
    "    if not es.indices.exists(index=index):\n",
    "        es.indices.create(index=index, body=mapp)\n",
    "        print(f\"L'index '{index}' a été créé avec succès.\")\n",
    "    else:\n",
    "        print(f\"L'index '{index}' existe déjà.\")\n",
    "    \n",
    "    # Supprimer tous les documents existants dans l'index\n",
    "    es.delete_by_query(index=index, body={\"query\": {\"match_all\": {}}})\n",
    "    \n",
    "    # Lire le fichier CSV et indexer les nouveaux documents\n",
    "    with open(f\"{os.path.dirname(os.getcwd())}/ML/{file}\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        \n",
    "        try:\n",
    "            helpers.bulk(es, reader, index=index)\n",
    "            print(\"Documents ajoutés avec succès.\")\n",
    "        except BulkIndexError as e:\n",
    "            print(f\"Erreur lors de l'indexation : {e.errors}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0750ae57-980b-4f76-8680-2cfe091174f1",
   "metadata": {},
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk, BulkIndexError \n",
    "import csv\n",
    "import os \n",
    "\n",
    "es = Elasticsearch(hosts = \"http://es-container:9200\", timeout=3000000)\n",
    "\n",
    "# Définir le nom de l'index\n",
    "index_name = \"test_final\"\n",
    "\n",
    "# Définir le mapping pour l'index\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"author_localisation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"author_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"author_url\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"experience_date\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd\"\n",
    "            },\n",
    "            \"extract_date\": {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd\"\n",
    "            },\n",
    "            \"firm_id\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"firm_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"note\": {\n",
    "                \"type\": \"float\"\n",
    "            },\n",
    "            \"reponse\": {\n",
    "                \"type\": \"boolean\"\n",
    "            },\n",
    "            \"review_date\": {\n",
    "                \"type\": \"date\"\n",
    "            },\n",
    "            \"review_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"review_title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"review_url\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"sentiments\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Créer l'index\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "    print(f\"L'index '{index_name}' a été créé avec succès.\")\n",
    "else:\n",
    "    print(f\"L'index '{index_name}' existe déjà.\")\n",
    "\n",
    "# Supprimer tous les documents existants dans l'index\n",
    "es.delete_by_query(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "\n",
    "# Lire le fichier CSV et indexer les nouveaux documents\n",
    "with open(f\"{os.path.dirname(os.getcwd())}/ML/data_final.csv\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    \n",
    "    try:\n",
    "        helpers.bulk(es, reader, index=index_name)\n",
    "        print(\"Documents ajoutés avec succès.\")\n",
    "    except BulkIndexError as e:\n",
    "        print(f\"Erreur lors de l'indexation : {e.errors}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e503489-ed52-4117-86ab-2c5eb144c435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
